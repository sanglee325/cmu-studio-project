{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "# define the transforms\n",
    "# This cell may need to be ran twice, ignore the first run error.\n",
    "transform = A.Compose([\n",
    "    A.Resize(128, 171, always_apply=True),\n",
    "    A.CenterCrop(112, 112, always_apply=True),\n",
    "    A.Normalize(mean = [0.43216, 0.394666, 0.37645],\n",
    "                std = [0.22803, 0.22145, 0.216989], \n",
    "                always_apply=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = \"action_recognition_kinetics.txt\"\n",
    "nn_model = \"resnet-34_kinetics.onnx\"\n",
    "video = \"example_activities.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import cv2\n",
    "import argparse\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('-f', '--file', required=False)\n",
    "parser.add_argument('-i', '--input', default = video, help='path to input video')\n",
    "parser.add_argument('-l', '--clip-len', dest='clip_len', default=16, type=int,\n",
    "                    help='number of frames to consider for each prediction')\n",
    "parser.add_argument('-c', '--classes', default = names, help='Path to classes list.')\n",
    "parser.add_argument('-m', '--model', default = nn_model, help='Path to model.')\n",
    "\n",
    "args = vars(parser.parse_args())\n",
    "#### PRINT INFO #####\n",
    "print(f\"Number of frames to consider for each prediction: {args['clip_len']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the lables\n",
    "class_names = open(args[\"classes\"]).read().strip().split(\"\\n\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# load the model\n",
    "model = torchvision.models.video.r3d_18(pretrained=True, progress=True) #This one works.\n",
    "\n",
    "# load the model onto the computation device\n",
    "model = model.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(args['input'])\n",
    "if (cap.isOpened() == False):\n",
    "    print('Error while trying to read video. Please check path again')\n",
    "# get the frame width and height\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name = f\"{args['input'].split('/')[-1].split('.')[0]}\"\n",
    "\n",
    "# define codec and create VideoWriter object \n",
    "fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n",
    "out = cv2.VideoWriter(f\"filepath/{save_name}.mp4\", fourcc, 15.0, (frame_width, frame_height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_count = 0 # to count total frames\n",
    "total_fps = 0 # to get the final frames per second\n",
    "# a clips list to append and store the individual frames\n",
    "clips = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read until end of video\n",
    "while(cap.isOpened()):\n",
    "    # capture each frame of the video\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:\n",
    "        # get the start time\n",
    "        start_time = time.time()\n",
    "        image = frame.copy()\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = transform(image=frame)['image']\n",
    "        clips.append(frame)\n",
    "        if len(clips) == args['clip_len']:\n",
    "            with torch.no_grad(): # we do not want to backprop any gradients\n",
    "                input_frames = np.array(clips)\n",
    "                # add an extra dimension        \n",
    "                input_frames = np.expand_dims(input_frames, axis=0)\n",
    "                # transpose to get [1, 3, num_clips, height, width]\n",
    "                input_frames = np.transpose(input_frames, (0, 4, 1, 2, 3))\n",
    "                # convert the frames to tensor\n",
    "                input_frames = torch.tensor(input_frames, dtype=torch.float32)\n",
    "                input_frames = input_frames.to(device)\n",
    "                # forward pass to get the predictions\n",
    "                outputs = model(input_frames)\n",
    "                # get the prediction index\n",
    "                _, preds = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # map predictions to the respective class names\n",
    "                label = class_names[preds].strip()\n",
    "            # get the end time\n",
    "            end_time = time.time()\n",
    "            # get the fps\n",
    "            fps = 1 / (end_time - start_time)\n",
    "            # add fps to total fps\n",
    "            total_fps += fps\n",
    "            # increment frame count\n",
    "            frame_count += 1\n",
    "            wait_time = max(1, int(fps/4))\n",
    "            cv2.putText(image, label, (15, 25),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2, \n",
    "                        lineType=cv2.LINE_AA)\n",
    "            clips.pop(0)\n",
    "            \n",
    "            #plt.imshow(image)\n",
    "            #plt.show()\n",
    "            out.write(image)\n",
    "            # press `q` to exit\n",
    "            if cv2.waitKey(wait_time) & 0xFF == ord('q'):\n",
    "                break\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# release VideoCapture()\n",
    "cap.release()\n",
    "# close all frames and video windows\n",
    "cv2.destroyAllWindows()\n",
    "# calculate and print the average FPS\n",
    "avg_fps = total_fps / frame_count\n",
    "print(f\"Average FPS: {avg_fps:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
