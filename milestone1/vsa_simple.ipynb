{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspired by https://github.com/opencv/opencv/blob/master/samples/dnn/action_recognition.py\n",
    "# and https://github.com/kenshohara/video-classification-3d-cnn-pytorch\n",
    "# and by https://www.pyimagesearch.com/2019/11/25/human-activity-recognition-with-opencv-and-deep-learning/\n",
    "\n",
    "# Altenative TensorFlow example https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import sys\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files - assumes these files are in the same file/directory as the Jupyter Notebook\n",
    "names = \"action_recognition_kinetics.txt\"\n",
    "model = \"resnet-34_kinetics.onnx\"\n",
    "video = \"example_activities.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(names) as l:\n",
    "    CLASSES = l.read().strip().split(\"\\n\")\n",
    "\n",
    "CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the contents of the class labels file, then define the sample\n",
    "# duration (i.e., # of frames for classification) and sample size\n",
    "# (i.e., the spatial dimensions of the frame)\n",
    "\n",
    "SAMPLE_DURATION = 16 \n",
    "SAMPLE_SIZE = 112 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the human activity recognition model\n",
    "print(\"[INFO] loading human activity recognition model...\")\n",
    "net = cv2.dnn.readNet(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab a pointer to the input video stream\n",
    "print(\"[INFO] accessing video stream...\")\n",
    "\n",
    "vs = cv2.VideoCapture(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the list / dictionaries to captured classified frames\n",
    "classifiedFrames = []\n",
    "framesDict = []\n",
    "dictKey = {}\n",
    "n = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model\n",
    "# loop until we explicitly break from it\n",
    "while True:\n",
    "  # initialize the batch of frames that will be passed through the model\n",
    "  frames = []\n",
    "\n",
    "  n += 1\n",
    "    \n",
    "  # loop over the number of required sample frames\n",
    "  for i in range(0, SAMPLE_DURATION):\n",
    "    # read a frame from the video stream\n",
    "    (grabbed, frame) = vs.read()\n",
    "\n",
    "    #Identiy the frame number\n",
    "    pos_frame = vs.get(cv2.CAP_PROP_POS_FRAMES)\n",
    "    \n",
    "    # if the frame was not grabbed then we've reached the end of\n",
    "    # the video stream so exit the script\n",
    "    if not grabbed:\n",
    "      print(\"[INFO] no frame read from stream - exiting\")\n",
    "      sys.exit(0)\n",
    "    # otherwise, the frame was read so resize it and add it to\n",
    "    # our frames list\n",
    "    frame = imutils.resize(frame, width=400)\n",
    "    frames.append(frame)\n",
    "  \n",
    "  # now that our frames array is filled we can construct our blob\n",
    "  blob = cv2.dnn.blobFromImages(frames, 1.0, (SAMPLE_SIZE, SAMPLE_SIZE), (114.7748, 107.7354, 99.4750), \n",
    "                                swapRB=True, crop=True)\n",
    "  blob = np.transpose(blob, (1, 0, 2, 3))\n",
    "  blob = np.expand_dims(blob, axis=0)\n",
    "  \n",
    "  # pass the blob through the network to obtain our human activity\n",
    "  # recognition predictions\n",
    "  net.setInput(blob)\n",
    "  outputs = net.forward()\n",
    "  label = CLASSES[np.argmax(outputs)]\n",
    "  \n",
    "  \n",
    "  #capture the frames and labels for future database search engine \n",
    "  classifiedFrames.append([pos_frame, label])\n",
    "  dictKey = {\"Index\": {\"_index\" : \"actions\", \"_id\" : n}}\n",
    "  value = {\"frame\" : pos_frame , \"activity\" : label}\n",
    "  framesDict.append(dictKey)\n",
    "  framesDict.append(value)\n",
    "\n",
    "\n",
    "  # loop over our frames\n",
    "  for i, frame in enumerate(frames):\n",
    "    if i % 50 == 0:\n",
    "      # draw the predicted activity on the frame\n",
    "      cv2.rectangle(frame, (0, 0), (300, 40), (0, 0, 0), -1) #300, 40\n",
    "      cv2.putText(frame, label, (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "      # display the frame to our screen\n",
    "      plt.imshow(frame)\n",
    "      plt.show()\n",
    "      \n",
    "      key = cv2.waitKey(1) & 0xFF\n",
    "      # if the `q` key was pressed, break from the loop\n",
    "      if key == ord(\"q\"):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# release VideoCapture()\n",
    "vs.release()\n",
    "# close all frames and video windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity = 'yoga'\n",
    "activity_in_list = [activity in list for list in classifiedFrames]\n",
    "\n",
    "for frame in classifiedFrames:\n",
    "  if activity in frame:\n",
    "    print(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "framesDict"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ab5325a40e878d66c76f034ece3ea9fcb7f70347e85ddb927190b439c9416f58"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('csp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
